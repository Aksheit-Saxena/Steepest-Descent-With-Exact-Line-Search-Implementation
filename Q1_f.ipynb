{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eFXx3d6Zxcta"
      },
      "outputs": [],
      "source": [
        "# Step 1: Coding up blackboxes for f, gradf, hessianf \n",
        "import numpy as np\n",
        "\n",
        "def f(x,Q,p):\n",
        "    first_term = np.dot(x, np.dot(Q, x.T))\n",
        "    # Calculate the second term p'*x\n",
        "    second_term = np.dot(p, x.T)\n",
        "    # Calculate the final result\n",
        "    result = 0.5 * first_term + second_term\n",
        "    return result\n",
        "\n",
        "def gradf(x,Q,p):\n",
        "    return Q@x.T+p\n",
        "\n",
        "def hessian(x,Q,p):\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNX0enN0dYNE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wxh4_ltOIdB",
        "outputId": "94a0ad01-9cdc-40d3-a7e6-592b928785b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration:  1 \n",
            "x:  [148.64697088   0.36676221]  f(x) 25973.18959631435 \n",
            " epsilon cond var 10000\n",
            "prev_x:  [1000    1]  f(prev_x):  600450.0 alpha= [-1.1e+03 -1.0e+00]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  2 \n",
            "x:  [-0.95771662  0.07381422]  f(x) -92.86120950823516 \n",
            " epsilon cond var 851.3532646252781\n",
            "prev_x:  [148.64697088   0.36676221]  f(prev_x):  25973.18959631435 alpha= [-248.64697088   -0.36676221]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  3 \n",
            "x:  [-8.32104641e+01  3.26962467e-02]  f(x) -4858.574671551389 \n",
            " epsilon cond var 149.6049743183675\n",
            "prev_x:  [-0.95771662  0.07381422]  f(prev_x):  -92.86120950823516 alpha= [-9.90422834e+01 -7.38142171e-02]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  4 \n",
            "x:  [-9.15308125e+01  4.29728644e-03]  f(x) -4964.128121346107 \n",
            " epsilon cond var 82.25275772091088\n",
            "prev_x:  [-8.32104641e+01  3.26962467e-02]  f(prev_x):  -4858.574671551389 alpha= [-16.78953593  -0.03269625]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  5 \n",
            "x:  [-9.92347219e+01  2.51220191e-03]  f(x) -4999.704334708698 \n",
            " epsilon cond var 8.320396873998025\n",
            "prev_x:  [-9.15308125e+01  4.29728644e-03]  f(prev_x):  -4964.128121346107 alpha= [-8.46918752e+00 -4.29728644e-03]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  6 \n",
            "x:  [-9.94800737e+01  1.32611407e-04]  f(x) -4999.86483039874 \n",
            " epsilon cond var 7.703909649883443\n",
            "prev_x:  [-9.92347219e+01  2.51220191e-03]  f(prev_x):  -4999.704334708698 alpha= [-0.76527808 -0.0025122 ]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  7 \n",
            "x:  [-9.99868201e+01  1.02941830e-04]  f(x) -4999.9999083763005 \n",
            " epsilon cond var 0.24536330217693225\n",
            "prev_x:  [-9.94800737e+01  1.32611407e-04]  f(prev_x):  -4999.86483039874 alpha= [-5.19926317e-01 -1.32611407e-04]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  8 \n",
            "x:  [-9.99886764e+01  1.02609299e-06]  f(x) -4999.999935887037 \n",
            " epsilon cond var 0.5067464060827801\n",
            "prev_x:  [-9.99868201e+01  1.02941830e-04]  f(prev_x):  -4999.9999083763005 alpha= [-0.01317991 -0.00010294]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  9 \n",
            "x:  [-9.99999625e+01  9.42688357e-07]  f(x) -4999.999999998898 \n",
            " epsilon cond var 0.0018590607307774965\n",
            "prev_x:  [-9.99886764e+01  1.02609299e-06]  f(prev_x):  -4999.999935887037 alpha= [-1.13236469e-02 -1.02609299e-06]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Iteration:  10 \n",
            "x:  [-9.99999642e+01  9.18008724e-10]  f(x) -4999.999999999358 \n",
            " epsilon cond var 0.011286177033222858\n",
            "prev_x:  [-9.99999625e+01  9.42688357e-07]  f(prev_x):  -4999.999999998898 alpha= [-3.74698847e-05 -9.42688357e-07]\n",
            "--------------------------------------------------------\n",
            "\n",
            "Converged! The minimial solution occurs @x =  [-1.00000000e+02  8.96838011e-10] \n",
            "with value f(x)= -4999.999999999999 \n",
            "and ||gradf||= 1.9026484883352295e-06\n",
            "\n",
            "Iteration:  11 \n",
            "x:  [-1.00000000e+02  8.96838011e-10]  f(x) -4999.999999999999 \n",
            " epsilon cond var 1.9026484883352295e-06\n",
            "prev_x:  [-1.00000000e+02  8.96838011e-10]  f(prev_x):  -4999.999999999999 alpha= [-3.58166637e-05 -9.18008724e-10]\n",
            "--------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-4999.999999999999"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "def gradient_descent_with_optimal_step_size_exact(f,x0,gradf,Q,p,e,convergence_thresh=0.12,max_iters=100,print_progress=True):\n",
        "    \"\"\"\n",
        "    @f is the function to be minimized\n",
        "    @gradf is the gradient of the function to be minimized\n",
        "    @x0 is the initial guess\n",
        "    @convergence_thresh is the threshold used to determine convergence\n",
        "    @max_iters is the maximum number of iteration we try before giving up\n",
        "    @print_progress flag to indicate whether to print the progress of the algorithm \n",
        "    \"\"\"\n",
        "    converged = False\n",
        "    num_iters_so_far = 0\n",
        "    x = x0\n",
        "    alpha = 0.2 # initial guess of step size\n",
        "    trajectory = []\n",
        "    while not converged:\n",
        "        # find descent direction\n",
        "        d = - gradf(x,Q,p) / np.linalg.norm(gradf(x,Q,p))\n",
        "        \n",
        "        # choose step size with exact line search!\n",
        "        one=np.linalg.inv(Q).T\n",
        "        two=-one@p.T-x.T\n",
        "        alpha=two/np.linalg.norm(d)\n",
        "        \n",
        "        \n",
        "        # update x\n",
        "        num_iters_so_far += 1\n",
        "        trajectory.append(x) \n",
        "        x = x - alpha * d\n",
        "        \n",
        "        # check convergence\n",
        "        \n",
        "        if num_iters_so_far<2:\n",
        "          grad_f_norm=10000\n",
        "        else:\n",
        "          grad_f_norm = np.linalg.norm(trajectory[-1]-trajectory[-2])\n",
        "\n",
        "        if grad_f_norm <= e:\n",
        "            converged = True\n",
        "            trajectory.append(x)\n",
        "            print(\"\\nConverged! The minimial solution occurs @x = \", x, \"\\nwith value f(x)=\",f(x,Q,p), \"\\nand ||gradf||=\",grad_f_norm)\n",
        "        if not converged and num_iters_so_far > max_iters:\n",
        "            converged = True\n",
        "            print(\"Failed to converge :(\")\n",
        "            \n",
        "        # output progress\n",
        "        if print_progress and num_iters_so_far % 1 == 0:\n",
        "            print(\"\\nIteration: \", num_iters_so_far,\"\\nx: \",x, \" f(x)\", f(x,Q,p), \"\\n epsilon cond var\",grad_f_norm) \n",
        "            print(\"prev_x: \", trajectory[-1], \" f(prev_x): \",f(trajectory[-1],Q,p),\"alpha=\",alpha)\n",
        "            print(\"--------------------------------------------------------\")\n",
        "    return x,np.array(trajectory)\n",
        "x = np.array([1000,1])\n",
        "e=10**-4\n",
        "v1 = np.array([1,0])\n",
        "v2 = np.array([0,900])\n",
        "Q = np.vstack((v1, v2))\n",
        "p=np.array([100,0])\n",
        "x_opt,trajectory = gradient_descent_with_optimal_step_size_exact(f,x,gradf,Q,p,e)\n",
        "f(x_opt,Q,p)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-b66svcBvfxB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}