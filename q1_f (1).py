# -*- coding: utf-8 -*-
"""Q1_f.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uosECrZWtDLAc9mt9Grzzlai-id_AiC-
"""

# Step 1: Coding up blackboxes for f, gradf, hessianf 
import numpy as np

def f(x,Q,p):
    first_term = np.dot(x, np.dot(Q, x.T))
    # Calculate the second term p'*x
    second_term = np.dot(p, x.T)
    # Calculate the final result
    result = 0.5 * first_term + second_term
    return result

def gradf(x,Q,p):
    return Q@x.T+p

def hessian(x,Q,p):
    return Q

def gradient_descent_with_optimal_step_size_exact(f,x0,gradf,Q,p,e,convergence_thresh=0.12,max_iters=100,print_progress=True):
    """
    @f is the function to be minimized
    @gradf is the gradient of the function to be minimized
    @x0 is the initial guess
    @convergence_thresh is the threshold used to determine convergence
    @max_iters is the maximum number of iteration we try before giving up
    @print_progress flag to indicate whether to print the progress of the algorithm 
    """
    converged = False
    num_iters_so_far = 0
    x = x0
    alpha = 0.2 # initial guess of step size
    trajectory = []
    while not converged:
        # find descent direction
        d = - gradf(x,Q,p) / np.linalg.norm(gradf(x,Q,p))
        
        # choose step size with exact line search!
        one=np.linalg.inv(Q).T
        two=-one@p.T-x.T
        alpha=two/np.linalg.norm(d)
        
        
        # update x
        num_iters_so_far += 1
        trajectory.append(x) 
        x = x - alpha * d
        
        # check convergence
        
        if num_iters_so_far<2:
          grad_f_norm=10000
        else:
          grad_f_norm = np.linalg.norm(trajectory[-1]-trajectory[-2])

        if grad_f_norm <= e:
            converged = True
            trajectory.append(x)
            print("\nConverged! The minimial solution occurs @x = ", x, "\nwith value f(x)=",f(x,Q,p), "\nand ||gradf||=",grad_f_norm)
        if not converged and num_iters_so_far > max_iters:
            converged = True
            print("Failed to converge :(")
            
        # output progress
        if print_progress and num_iters_so_far % 1 == 0:
            print("\nIteration: ", num_iters_so_far,"\nx: ",x, " f(x)", f(x,Q,p), "\n epsilon cond var",grad_f_norm) 
            print("prev_x: ", trajectory[-1], " f(prev_x): ",f(trajectory[-1],Q,p),"alpha=",alpha)
            print("--------------------------------------------------------")
    return x,np.array(trajectory)
x = np.array([1000,1])
e=10**-4
v1 = np.array([1,0])
v2 = np.array([0,900])
Q = np.vstack((v1, v2))
p=np.array([100,0])
x_opt,trajectory = gradient_descent_with_optimal_step_size_exact(f,x,gradf,Q,p,e)
f(x_opt,Q,p)

